{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Size, Tensor\n",
    "from torch.distributions import Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/remote/gpu04/hummerich/mol-fff/.venv/lib/python3.11/site-packages/torch_geometric/data/dataset.py:240: UserWarning: The `pre_transform` argument differs from the one used in the pre-processed version of this dataset. If you want to make use of another pre-processing technique, pass `force_reload=True` explicitly to reload the dataset.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from yaml import safe_load\n",
    "from core.data.unimers import UnimersData\n",
    "\n",
    "data_hparams = safe_load(open(\"/remote/gpu04/hummerich/mol-fff/mol-fff/config/unimers_autoencoder.yaml\"))[\n",
    "    \"data_hparams\"\n",
    "]\n",
    "data = UnimersData(UnimersData.hparams_schema.from_dict(data_hparams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Any, Callable\n",
    "\n",
    "from rdkit.Chem.Draw import MolsToGridImage\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.utils import unbatch\n",
    "\n",
    "from hydrantic.model import ModelHparams, Model\n",
    "\n",
    "from fff.loss import volume_change_surrogate\n",
    "from fff.utils.func import compute_jacobian, compute_volume_change\n",
    "from fff.utils.utils import sum_except_batch\n",
    "\n",
    "from core.models.graph_autoencoder import GraphAutoencoder\n",
    "from core.mixins.length_encoding import LengthEncodingMixin\n",
    "from core.components.set_attention import SAB\n",
    "from core.components.set_rff import RFF\n",
    "from core.utils.masking import pad_sets, unpad_sets, apply_masks\n",
    "from core.utils.graphs import dense_edge_index\n",
    "from core.utils.molecules import get_molecule_from_data\n",
    "from core.utils.sets import length_encoding\n",
    "from core.metrics.molecules import Validity, Components, Uniqueness\n",
    "from core.metrics.mmd import MMD\n",
    "\n",
    "\n",
    "class NodeEmbeddingFFFHparams(ModelHparams):\n",
    "    dim: int\n",
    "    latent_dim: int\n",
    "    hidden_dim: int\n",
    "    num_bead_classes: int\n",
    "    mlp_widths: list[int]\n",
    "    heads: int\n",
    "\n",
    "    graph_autoencoder_ckpt: str\n",
    "\n",
    "    latent_distribution: Literal[\"normal\"] = \"normal\"\n",
    "    num_blocks: int\n",
    "\n",
    "    beta: float\n",
    "\n",
    "\n",
    "class NodeEmbeddingFFF(Model, LengthEncodingMixin):\n",
    "    hparams_schema = NodeEmbeddingFFFHparams\n",
    "    graph_autoencoder: GraphAutoencoder\n",
    "\n",
    "    class PermutationEquivariantLayer(nn.Module):\n",
    "        def __init__(\n",
    "            self,\n",
    "            input_dim: int,\n",
    "            output_dim: int,\n",
    "            hidden_dim: int,\n",
    "            heads: int,\n",
    "            mlp_widths: list[int],\n",
    "        ):\n",
    "            super().__init__()\n",
    "            self.state_update = RFF(2 * hidden_dim, hidden_dim, mlp_widths, activation=\"torch.nn.ELU\")\n",
    "            self.interaction_sab = SAB(input_dim + hidden_dim, hidden_dim, heads)\n",
    "            self.observation = RFF(hidden_dim, output_dim, mlp_widths, activation=\"torch.nn.ELU\")\n",
    "\n",
    "        def forward(self, x: Tensor, s: Tensor, lengths: Tensor) -> tuple[Tensor, Tensor]:\n",
    "            m = self.interaction_sab(torch.concat((s, x), dim=-1), lengths)\n",
    "            x = x + self.observation(m, lengths)\n",
    "            s = self.state_update(torch.concat((s, m), dim=-1), lengths)\n",
    "            return x, s\n",
    "\n",
    "    class ConditionalNormalDistribution(nn.Module, Distribution):\n",
    "        def __init__(self, dim: int, condition_dim: int):\n",
    "            super().__init__()\n",
    "            self.mu_embedding = nn.Linear(condition_dim, dim)\n",
    "            self.sigma_embedding = nn.Linear(condition_dim, dim)\n",
    "            self.dim = dim\n",
    "            self.condition_dim = condition_dim\n",
    "\n",
    "        def sample(self, sample_shape: Size, condition: Tensor):\n",
    "            mu = self.mu_embedding(condition)\n",
    "            sigma = self.sigma_embedding(condition).exp()\n",
    "            eps = torch.randn(sample_shape + (self.dim,), dtype=condition.dtype, device=condition.device)\n",
    "            return mu + sigma * eps\n",
    "\n",
    "        def log_prob(self, x: Tensor, condition: Tensor):\n",
    "            mu = self.mu_embedding(condition)\n",
    "            sigma = self.sigma_embedding(condition).exp()\n",
    "            log_p = (\n",
    "                -(self.dim / 2) * torch.log(2 * torch.tensor(torch.pi))\n",
    "                - (self.dim / 2) * torch.log(sigma**2)\n",
    "                - (1 / (2 * sigma**2)) * (x - mu) ** 2\n",
    "            )\n",
    "            return log_p.sum(dim=-1)\n",
    "\n",
    "    def __init__(self, hparams):\n",
    "        super().__init__(hparams)\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [\n",
    "                self.PermutationEquivariantLayer(\n",
    "                    input_dim=self.hparams.dim,\n",
    "                    output_dim=self.hparams.dim,\n",
    "                    hidden_dim=self.hparams.hidden_dim,\n",
    "                    heads=self.hparams.heads,\n",
    "                    mlp_widths=self.hparams.mlp_widths,\n",
    "                )\n",
    "                for _ in range(self.hparams.num_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [\n",
    "                self.PermutationEquivariantLayer(\n",
    "                    input_dim=self.hparams.dim,\n",
    "                    output_dim=self.hparams.dim,\n",
    "                    hidden_dim=self.hparams.hidden_dim,\n",
    "                    heads=self.hparams.heads,\n",
    "                    mlp_widths=self.hparams.mlp_widths,\n",
    "                )\n",
    "                for _ in range(self.hparams.num_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.bead_embedding = nn.Linear(self.hparams.num_bead_classes, self.hparams.hidden_dim)\n",
    "        self.graph_autoencoder = self._configure_graph_autoencoder()\n",
    "        self.latent_distribution = self.ConditionalNormalDistribution(\n",
    "            self.hparams.latent_dim, self.hparams.num_bead_classes\n",
    "        )\n",
    "\n",
    "    def compute_metrics(self, batch: Any, batch_idx: int) -> dict[str, Tensor]:\n",
    "        metrics: dict[str, Tensor] = {}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            h, lengths = self.embed(batch)\n",
    "\n",
    "        loss = torch.zeros(h.shape[0], device=h.device, dtype=h.dtype)\n",
    "\n",
    "        c = batch.bead.unsqueeze(1).repeat(1, h.size(1), 1)\n",
    "\n",
    "        if self.training:\n",
    "            h.requires_grad_()\n",
    "            z, h1, nll = self.nll_surrogate(h, lengths, c=c)\n",
    "            metrics[\"nll\"] = sum_except_batch(nll).mean()\n",
    "            mse = sum_except_batch((h - h1).pow(2))\n",
    "            metrics[\"mse\"] = mse.mean()\n",
    "            loss += nll + self.hparams.beta * mse\n",
    "\n",
    "        else:\n",
    "            z, mmd = self.latent_mmd(h, lengths, c=c)\n",
    "            h1 = self.decode(z, lengths, c=c)\n",
    "            mse = sum_except_batch((h - h1).pow(2))\n",
    "            metrics[\"mse\"] = mse.mean()\n",
    "            metrics[\"mmd\"] = mmd\n",
    "            loss += mmd\n",
    "\n",
    "            if batch_idx == 0:\n",
    "                z_gen = self.latent_distribution.sample(z.shape[:-1], c)\n",
    "                h_gen = self.decode(z_gen, lengths, c=c)\n",
    "                batch_gen = self.reconstruct(h_gen, lengths)\n",
    "                mols_gen = [\n",
    "                    get_molecule_from_data(\n",
    "                        batch_gen[i].x,\n",
    "                        batch_gen[i].edge_index,\n",
    "                        batch_gen[i].edge_attr,\n",
    "                        profile=self.graph_autoencoder.hparams.profile,\n",
    "                    )\n",
    "                    for i in range(batch.num_graphs)\n",
    "                ]\n",
    "                print(self.graph_autoencoder.hparams.profile)\n",
    "                metrics[\"validity\"] = torch.tensor(Validity()(mols_gen))\n",
    "                metrics[\"components\"] = torch.tensor(Components()(mols_gen))\n",
    "                metrics[\"uniqueness\"] = torch.tensor(Uniqueness()(mols_gen))\n",
    "\n",
    "                try:\n",
    "                    self.logger.log_image(\n",
    "                        \"mols_gen\",\n",
    "                        [MolsToGridImage(mols_gen[: min(len(mols_gen), 64)], molsPerRow=8)],\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "        metrics[\"loss\"] = loss.mean()\n",
    "\n",
    "        return metrics\n",
    "\n",
    "    def embed(self, batch: Batch) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"Embeds a batch of graphs as a set of node embeddings.\n",
    "\n",
    "        :param batch: A batch of graphs.\n",
    "        :return: A batch of node embeddings and the corresponding lengths.\"\"\"\n",
    "\n",
    "        h_graph = self.graph_autoencoder.encode(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "        h_list = unbatch(h_graph, batch.batch)\n",
    "        h, lengths = pad_sets(h_list)\n",
    "        return h, lengths\n",
    "\n",
    "    def reconstruct(self, h: Tensor, lengths) -> Batch:\n",
    "        \"\"\"Reconstructs the graph structures from a set of node embeddings.\n",
    "\n",
    "        :param h: A batch of node embeddings.\n",
    "        :param lengths: A batch of lengths.\n",
    "        :return: A batch of graph structures.\"\"\"\n",
    "\n",
    "        batch_code = self.set_to_graph(h, lengths)\n",
    "        x1, edge_attr1 = self.graph_autoencoder.decode(batch_code.x, batch_code.edge_index, batch_code.batch)\n",
    "        batch1 = batch_code.clone()\n",
    "        batch1.x = x1\n",
    "        batch1.edge_attr = edge_attr1\n",
    "        return batch1\n",
    "\n",
    "    def encode(self, h: Tensor, lengths: Tensor, c: Tensor | None = None) -> Tensor:\n",
    "        \"\"\"Encodes a batch a node embeddings to its latent representation.\n",
    "\n",
    "        :param h: A batch of node embeddings.\n",
    "        :param lengths: A batch of lengths.\n",
    "        :param c: Conditions\n",
    "        :return: A batch of latent node embeddings.\"\"\"\n",
    "\n",
    "        z = h\n",
    "        s = self.bead_embedding(c)\n",
    "        for layer in self.encoder_layers:\n",
    "            z, s = layer(z, s, lengths)\n",
    "        return z\n",
    "\n",
    "    def decode(self, z: Tensor, lengths: Tensor, c: Tensor | None = None) -> Tensor:\n",
    "        \"\"\"Decodes a batch of latent representations to a batch of node embeddings.\n",
    "\n",
    "        :param z: A batch of node embeddings.\n",
    "        :param lengths: A batch of lengths.\n",
    "        :param c: Conditions\n",
    "        :return: A batch of node embeddings\"\"\"\n",
    "\n",
    "        h = z\n",
    "        s = self.bead_embedding(c)\n",
    "        for layer in self.decoder_layers:\n",
    "            h, s = layer(h, s, lengths)\n",
    "        return h\n",
    "\n",
    "    def init_hidden_state(\n",
    "        self,\n",
    "        lengths: Tensor,\n",
    "        shape: torch.Size,\n",
    "        c: Tensor | None = None,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "        dtype: torch.dtype = torch.float32,\n",
    "    ) -> Tensor:\n",
    "        batch_size, set_size = shape\n",
    "        le = length_encoding(lengths, self.hparams.length_encoding_dim, device, dtype).unsqueeze(1)\n",
    "        rnf = torch.randn((batch_size, 1, self.hparams.rnf_dim), device=device, dtype=dtype)\n",
    "        s = torch.concat((le.repeat(1, set_size, 1), rnf.repeat(1, set_size, 1)), dim=-1)\n",
    "        if c is not None:\n",
    "            s = torch.concat((s, c.repeat(1, set_size, 1)), dim=-1)\n",
    "        return s\n",
    "\n",
    "    def _configure_graph_autoencoder(self) -> GraphAutoencoder:\n",
    "        graph_autoencoder = GraphAutoencoder.load_from_checkpoint(\n",
    "            self.hparams.graph_autoencoder_ckpt, map_location=self.device\n",
    "        )\n",
    "        graph_autoencoder = graph_autoencoder.eval()\n",
    "        graph_autoencoder = graph_autoencoder.requires_grad_(False)\n",
    "        graph_autoencoder.freeze()\n",
    "        return graph_autoencoder\n",
    "\n",
    "    @property\n",
    "    def num_bond_classes(self) -> int:\n",
    "        \"\"\"Number of bond classes.\"\"\"\n",
    "        num_edge_classes: dict[str, int] = {\n",
    "            \"qm9\": 5,\n",
    "            \"pcqm4m\": 5,\n",
    "            \"zinc\": 4,\n",
    "            \"unimers\": 4,\n",
    "        }\n",
    "        return num_edge_classes[self.graph_autoencoder.hparams.profile]\n",
    "\n",
    "    @property\n",
    "    def num_atom_classes(self) -> int:\n",
    "        \"\"\"Number of atom classes.\"\"\"\n",
    "        num_atom_classes: dict[str, int] = {\n",
    "            \"qm9\": 9,\n",
    "            \"pcqm4m\": 36,\n",
    "            \"zinc\": 28,\n",
    "            \"unimers\": 4,\n",
    "        }\n",
    "        return num_atom_classes[self.graph_autoencoder.hparams.profile]\n",
    "\n",
    "    def graph_to_set(self, batch: Batch) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"Transforms a batch of graphs to a batch of sets.\n",
    "\n",
    "        :param batch: batch of graphs to transform\n",
    "        :return: batch of sets\"\"\"\n",
    "\n",
    "        x_list = unbatch(batch.x, batch.batch)\n",
    "        x_list, lengths = pad_sets(x_list)\n",
    "        return x_list, lengths\n",
    "\n",
    "    def set_to_graph(self, x: Tensor, lengths: Tensor) -> Batch:\n",
    "        \"\"\"Transforms a batch of sets to a batch of fully connected graphs with empty edge attributes.\n",
    "\n",
    "        :param x: batch of sets to transform\n",
    "        :param lengths: batch of sets lengths\n",
    "        :return: batch of sets\"\"\"\n",
    "\n",
    "        x_list = unpad_sets(x, lengths)\n",
    "        geom_data_list = [\n",
    "            Data(\n",
    "                x=x_list[i],\n",
    "                edge_index=dense_edge_index(lengths[i].item(), x.device),\n",
    "                edge_attr=torch.empty(\n",
    "                    (lengths[i] ** 2 - lengths[i], self.num_bond_classes),\n",
    "                    device=x.device,\n",
    "                ),\n",
    "            )\n",
    "            for i in range(len(x_list))\n",
    "        ]\n",
    "        return Batch.from_data_list(geom_data_list)\n",
    "\n",
    "    def _get_encode_prefilled(self, lengths: Tensor, c: Tensor | None = None) -> Callable[[Tensor], Tensor]:\n",
    "        return lambda x: self.encode(x, lengths, c)\n",
    "\n",
    "    def _get_decode_prefilled(self, lengths: Tensor, c: Tensor | None = None) -> Callable[[Tensor], Tensor]:\n",
    "        return lambda z: self.decode(z, lengths, c)\n",
    "\n",
    "    def nll_surrogate(self, x: Tensor, lengths: Tensor, c: Tensor | None = None) -> tuple[Tensor, Tensor, Tensor]:\n",
    "        \"\"\"Computes the negative log likelihood using the volume change surrogate.\n",
    "\n",
    "        :param x: The input tensor.\n",
    "        :param lengths: The lengths of the sets.\n",
    "        :param c: The condition tensor.\n",
    "        :return: The latent code, the reconstruction and the negative log-likelihood surrogate.\n",
    "        \"\"\"\n",
    "\n",
    "        encode_prefilled = self._get_encode_prefilled(lengths, c)\n",
    "        decode_prefilled = self._get_decode_prefilled(lengths, c)\n",
    "\n",
    "        surrogate_output = volume_change_surrogate(x, encode_prefilled, decode_prefilled)\n",
    "\n",
    "        z = surrogate_output.z\n",
    "        x1 = surrogate_output.x1\n",
    "\n",
    "        nll = -self.assemble_log_prob(z, surrogate_output.surrogate, lengths, c)\n",
    "\n",
    "        return z, x1, nll\n",
    "\n",
    "    def nll_exact(self, x: Tensor, lengths: Tensor, c: Tensor | None = None) -> tuple[Tensor, Tensor, Tensor]:\n",
    "        \"\"\"Computes the exact negative log likelihood of the decoder.\n",
    "\n",
    "        :param x: The input tensor.\n",
    "        :param lengths: The lengths of the sets.\n",
    "        :param c: The condition tensor.\n",
    "        :return: The latent code, the reconstruction and the exact negative log-likelihood.\n",
    "        \"\"\"\n",
    "\n",
    "        encode_prefilled = self._get_encode_prefilled(lengths, c)\n",
    "\n",
    "        vmap_args = (lengths, c) if c is not None else (lengths,)\n",
    "\n",
    "        z = encode_prefilled(x)\n",
    "\n",
    "        x1, jac = compute_jacobian(\n",
    "            z,\n",
    "            self.decode,\n",
    "            *vmap_args,\n",
    "            chunk_size=self.hparams.chunk_size,  # type: ignore\n",
    "            grad_type=\"backward\",\n",
    "        )\n",
    "        vol_change = compute_volume_change(\n",
    "            jac.view(jac.shape[0], jac.shape[1] * jac.shape[2], jac.shape[3] * jac.shape[4])\n",
    "        )\n",
    "\n",
    "        # use the negative volume change of the decoder to assemble the log prob\n",
    "        nll = -self.assemble_log_prob(z, -vol_change, lengths, c)\n",
    "\n",
    "        return z, x1, nll\n",
    "\n",
    "    def assemble_log_prob(self, z: Tensor, vol_change: Tensor, lengths: Tensor, c: Tensor | None = None) -> Tensor:\n",
    "        \"\"\"Assembles the log probability given the volume change and the lengths.\n",
    "\n",
    "        :param z: The latent tensor.\n",
    "        :param vol_change: The volume change tensor.\n",
    "        :param lengths: The lengths of the sets.\n",
    "        :return: The log probability tensor.\"\"\"\n",
    "\n",
    "        original_shape = z.shape\n",
    "        # z = z.view(-1, self.hparams.latent_dim)\n",
    "        log_prob = self.latent_distribution.log_prob(z, c)\n",
    "\n",
    "        log_prob = log_prob.view(*original_shape[:-1], -1)  # reshape to original\n",
    "        log_prob = apply_masks(log_prob, lengths)  # mask out the padded elements\n",
    "        log_prob = sum_except_batch(log_prob)  # sum over elements\n",
    "        log_prob += vol_change  # add the set wise volume change\n",
    "\n",
    "        return log_prob\n",
    "\n",
    "    def latent_mmd(self, x: Tensor, lengths: Tensor, c: Tensor | None = None) -> tuple[Tensor, Tensor]:\n",
    "        \"\"\"Computes the latent MMD of the model, which can be used to monitor the learned latent distribution.\n",
    "\n",
    "        :param x: The input tensor.\n",
    "        :param lengths: The lengths of the sets.\n",
    "        :param c: The condition tensor.\n",
    "        :return: The latent code tensor and the mmd value.\"\"\"\n",
    "\n",
    "        z = self.encode(x, lengths, c)\n",
    "        z_sampled = self.sample_z(z.shape[:-1], z.device, z.dtype)\n",
    "        mmd = MMD(\n",
    "            kernel=\"multiscale\",\n",
    "            bandwidth_range=[0.3, 0.5, 0.8, 1.0, 1.5, 2.0, 3.0, 5.0],\n",
    "        )(z.reshape(z.shape[0], -1), z_sampled.view(z_sampled.shape[0], -1))\n",
    "        return z, mmd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = NodeEmbeddingFFFHparams(\n",
    "    dim=10,\n",
    "    latent_dim=10,\n",
    "    hidden_dim=32,\n",
    "    num_bead_classes=26,\n",
    "    mlp_widths=[32],\n",
    "    heads=1,\n",
    "    graph_autoencoder_ckpt=\"/remote/gpu04/hummerich/mol-fff/mol-fff/lightning_logs/Selected Models/GraphAutoencoder_dim10_regularized.ckpt\",\n",
    "    latent_distribution=\"normal\",\n",
    "    num_blocks=4,\n",
    "    beta=1000,\n",
    ")\n",
    "\n",
    "model = NodeEmbeddingFFF(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/remote/gpu04/hummerich/mol-fff/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "/remote/gpu04/hummerich/mol-fff/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/configuration_validator.py:70: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "/remote/gpu04/hummerich/mol-fff/.venv/lib/python3.11/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:654: Checkpoint directory /remote/gpu04/hummerich/mol-fff/mol-fff/checkpoints exists and is not empty.\n",
      "\n",
      "  | Name                | Type                          | Params | Mode \n",
      "------------------------------------------------------------------------------\n",
      "0 | encoder_layers      | ModuleList                    | 39.3 K | train\n",
      "1 | decoder_layers      | ModuleList                    | 39.3 K | train\n",
      "2 | bead_embedding      | Linear                        | 864    | train\n",
      "3 | graph_autoencoder   | GraphAutoencoder              | 82.2 K | eval \n",
      "4 | latent_distribution | ConditionalNormalDistribution | 540    | train\n",
      "------------------------------------------------------------------------------\n",
      "80.1 K    Trainable params\n",
      "82.2 K    Non-trainable params\n",
      "162 K     Total params\n",
      "0.649     Total estimated model params size (MB)\n",
      "206       Modules in train mode\n",
      "155       Modules in eval mode\n",
      "/remote/gpu04/hummerich/mol-fff/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1156 [03:03<?, ?it/s]5.15it/s, training/loss=1.61e+4]\n",
      "Epoch 0:   0%|          | 0/1156 [01:20<?, ?it/s]\n",
      "Epoch 0:   0%|          | 0/1156 [00:55<?, ?it/s]\n",
      "Epoch 0:  52%|█████▏    | 600/1156 [01:50<01:42,  5.43it/s, training/loss=1.33e+3]"
     ]
    }
   ],
   "source": [
    "model.fit_fast(data.train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
