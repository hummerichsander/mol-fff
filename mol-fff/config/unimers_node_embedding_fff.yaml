model_hparams:
  module_name: core.models.node_embedding_fff.NodeEmbeddingFFF

  dim: 10
  latent_dim: 10
  hidden_dim: 32
  mlp_widths: [ 32 ]
  heads: 2
  graph_autoencoder_ckpt: /Users/hummerichsander/PycharmProjects/mol-fff/mol-fff/lightning_logs/Selected Checkpoints/GraphAutoencoder_dim10.ckpt
  latent_distribution: normal
  num_blocks: 6
  beta: 1000

  optimizer:
    module_name: torch.optim.Adam
    kwargs:
      lr: 0.0002
      weight_decay: 0.001

  scheduler:
    module_name: torch.optim.lr_scheduler.ExponentialLR
    kwargs:
      gamma: 0.999

  checkpoint:
    monitor: validation/loss
    save_last: true  # always checkpoints the latest run
    save_top_k: 5  # saves the best 5 runs
    save_every_n_epochs: 1  # evaluate checkpoints every epoch

run_hparams:
  trainer:
    accelerator: cpu  # use gpu
    max_epochs: 100  # number of epochs to train
    precision: 32  # model weight precision (should be the same as dataset precision)
    limit_train_batches: 0.1
    limit_val_batches: 0.1

  logger:
    project: MolFFF  # name of the project in your w&b profile
    log_model: false  # whether to log the model to w&b
    offline: false  # whether to upload the logs to w&b

data_hparams:
  module_name: "core.data.UnimersData"
  loader:
    batch_size: 64
    shuffle: False
    pin_memory: False
    drop_last: False
    num_workers: 0
    persistent_workers: False
  split: [ 0.8, 0.1, 0.1 ]  # dataset split into train, validation (and test)
  seed: 42  # Seed to use to split the dataset
  root: "./data/unimers_float32"
  pre_transform:
    - [ core.data.transforms.OneHotEncode, { attrs: [ "x" ], num_classes: [ 4 ], shift: -6 } ]
    - [ core.data.transforms.OneHotEncode, { attrs: [ "edge_attr" ], num_classes: [ 3 ], shift: -1 } ]
    - [ core.data.transforms.AddEmptyEdges, { } ]

hydra:
  job:
    name: Unimers_Flow
  run:
    dir: lightning_logs/${hydra.job.name}/${now:%Y-%m-%d_%H-%M-%S}
  sweep:
    dir: lightning_logs/${hydra.job.name}
    subdir: ${hydra.job.override_dirname}
